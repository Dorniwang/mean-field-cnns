{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Delta Orthogonal Convolution Tutorial",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "/piper/depot/google3/experimental/users/xlc/tutorial/delta-orthogonal/Delta_Orthogonal_Convolution_Tutorial.ipynb?workspaceId=xlc:delta-tutorial::citc",
          "timestamp": 1530912596374
        },
        {
          "file_id": "1jCrtgpsARtzdK15DU3n_Fg7sg3i8Z_zH",
          "timestamp": 1530817417472
        }
      ],
      "collapsed_sections": [
        "yM4dNJo5vTNK"
      ]
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "yM4dNJo5vTNK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##### Copyright 2018 Google LLC.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "metadata": {
        "id": "gXsmXsnSvSWg",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Delta Orthogonal Convolution Tutorial\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TgkHGgykhOgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Initialization"
      ]
    },
    {
      "metadata": {
        "id": "zkS604HdvdOc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from scipy.integrate import quad, dblquad\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d6bOit_9hky-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Delta Orthogonal Network"
      ]
    },
    {
      "metadata": {
        "id": "Ly8JGExPqG6b",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "DEPTH = 50 # number of layers.\n",
        "C_SIZE = 64 # channel size.\n",
        "K_SIZE = 3 # kernel size  # bias variance.\n",
        "LEARNING_RATE = 1e-3\n",
        "phi = tf.tanh # non-linearity\n",
        "\n",
        "# variances of weight and bias. \n",
        "# To obtain critical values of the variances of the weights and biases,\n",
        "# see compute mean field below.  \n",
        "W_VAR, B_VAR = 1.05, 2.01e-5 \n",
        "\n",
        "\n",
        "def circular_padding(input_, width, kernel_size):\n",
        "  \"\"\"Padding input_ for computing circular convolution.\"\"\"\n",
        "  begin = kernel_size // 2\n",
        "  end = kernel_size - 1 - begin\n",
        "  tmp_up = tf.slice(input_, [0, width - begin, 0, 0], [-1, begin, width, -1])\n",
        "  tmp_down = tf.slice(input_, [0, 0, 0, 0], [-1, end, width, -1])\n",
        "  tmp = tf.concat([tmp_up, input_, tmp_down], 1)\n",
        "  new_width = width + kernel_size - 1\n",
        "  tmp_left = tf.slice(tmp, [0, 0, width - begin, 0], [-1, new_width, begin, -1])\n",
        "  tmp_right = tf.slice(tmp, [0, 0, 0, 0], [-1, new_width, end, -1])\n",
        "  return  tf.concat([tmp_left, tmp, tmp_right], 2)\n",
        "\n",
        "def conv2d(x, w, strides=1, padding='SAME'):\n",
        "  return tf.nn.conv2d(x, w, strides=[1, strides, strides, 1], padding=padding)\n",
        "\n",
        "def get_weight(shape, std, name=None):\n",
        "  return tf.Variable(tf.random_normal(shape, mean=0, stddev=std), name=name)\n",
        "\n",
        "def get_orthogonal_weight(name, shape, std=1):\n",
        "  # Can also use tf.contrib.framework.convolutional_orthogonal_2d\n",
        "  return tf.get_variable(name, shape=shape,\n",
        "     initializer=tf.contrib.framework.convolutional_delta_orthogonal(gain=std))\n",
        "\n",
        "def get_weight(shape, std, name=None):\n",
        "  return tf.Variable(tf.random_normal(shape, mean=0, stddev=std), name=name)\n",
        "\n",
        "def conv_model(x):\n",
        "  \"\"\"Convolutional layers. Ouput logits. \"\"\"\n",
        "  z = tf.reshape(x, [-1,28,28,1])\n",
        "  # Increase the channel size to C_SIZE.\n",
        "  std = np.sqrt(W_VAR / (K_SIZE**2 * 1))\n",
        "  kernel = get_weight([K_SIZE, K_SIZE, 1, C_SIZE], std=std, name='kernel_0')\n",
        "  bias = get_weight([C_SIZE], std=np.sqrt(B_VAR), name='bias_0')\n",
        "  h = conv2d(z, kernel, strides=1, padding='SAME') + bias\n",
        "  z = phi(h)\n",
        "  \n",
        "  # Reducing spacial dimension to 7 * 7; applying conv with stride=2 twice.\n",
        "  std = np.sqrt(W_VAR / (K_SIZE**2 * C_SIZE))\n",
        "  shape = [K_SIZE, K_SIZE, C_SIZE, C_SIZE]\n",
        "  for j in range(2):\n",
        "    kernel = get_weight(shape, std=std, name='reduction_{}_kernel'.format(j))\n",
        "    bias = get_weight([C_SIZE], std=np.sqrt(B_VAR),\n",
        "                      name='reduction_{}_bias'.format(j))\n",
        "    h = conv2d(z, kernel, strides=2) + bias\n",
        "    z = phi(h)\n",
        "  new_width = 7 # width of the current image after dimension reduction. \n",
        "  \n",
        "  # A deep convolution block with depth=DEPTH.\n",
        "  for j in range(DEPTH):\n",
        "    name = 'block_conv_{}'.format(j)\n",
        "    kernel_name, bias_name = name + 'kernel', name + 'bias'\n",
        "    kernel = get_orthogonal_weight(kernel_name, shape, std=np.sqrt(W_VAR))\n",
        "    bias = get_weight([C_SIZE], std=np.sqrt(B_VAR), name=bias_name)\n",
        "    z_pad = circular_padding(z, new_width, K_SIZE)\n",
        "    h = conv2d(z_pad, kernel, padding='VALID') + bias\n",
        "    z = phi(h)\n",
        "  z_ave = tf.reduce_mean(z, [1, 2])\n",
        "  weight_to_classes = get_weight([C_SIZE, 10], np.sqrt(W_VAR/C_SIZE))\n",
        "  return tf.matmul(z_ave, weight_to_classes) + get_weight([10], np.sqrt(B_VAR))\n",
        "\n",
        "def loss(logits, labels):\n",
        "  return tf.reduce_mean(\n",
        "      tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels))\n",
        "\n",
        "def train_op(loss_, learning_rate, global_step):\n",
        "  with tf.control_dependencies([tf.assign(global_step, global_step + 1)]):\n",
        "    return tf.train.MomentumOptimizer(learning_rate, 0.9).minimize(loss_)\n",
        "\n",
        "def accuracy(logits, labels):\n",
        "  return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1),\n",
        "                                         tf.argmax(labels, 1)), \n",
        "                                tf.float32))\n",
        "\n",
        "def run_model(num_steps=1000):\n",
        "  tf.reset_default_graph()\n",
        "  x = tf.placeholder(tf.float32, shape=[None, 784])\n",
        "  y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
        "  global_step = tf.train.get_or_create_global_step()\n",
        "  logits = conv_model(x)\n",
        "  acc, loss_ = accuracy(logits, y_), loss(logits, y_)\n",
        "  training = train_op(loss_, LEARNING_RATE, global_step)\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(num_steps):\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "      _, acc_value, loss_value, g_step = sess.run(\n",
        "          [training, acc, loss_, global_step], \n",
        "          feed_dict={x:batch_xs, y_:batch_ys})\n",
        "      if i % 10 == 0: \n",
        "        print('Step:', g_step, 'Accuracy:', acc_value,'Loss:', loss_value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mi0I2nqPK1pW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "run_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AQDA6OyOK2mT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions and class for mean field calculations."
      ]
    },
    {
      "metadata": {
        "id": "z6Ps0zhZF2yT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def gauss_density(x):\n",
        "  return np.exp(-x**2/2)/np.sqrt(2*np.pi)\n",
        "\n",
        "\n",
        "class MeanField(object):\n",
        "  def __init__(self, f, df):\n",
        "    \"\"\"\n",
        "    Args: \n",
        "      f: activation function\n",
        "      df: derivative of f. \n",
        "    \"\"\"\n",
        "    self.f = f\n",
        "    self.df = df\n",
        "  \n",
        "  def qmap_density(self, x, q):\n",
        "    \"\"\"Compute the density function of the q-map.\"\"\"\n",
        "    return (self.f(np.sqrt(q)*x)**2)* gauss_density(x)\n",
        "\n",
        "  \n",
        "  def chi_density(self, x, q):\n",
        "    return gauss_density(x) * (self.df(np.sqrt(q)*x)**2)\n",
        "  \n",
        "  \n",
        "  def sw_sb(self, q, chi1):\n",
        "    \"\"\"Compute the critical line. Set chi1=1, return variances of weight and \n",
        "       bias on the critical line with qstar=q.\n",
        "    \"\"\"\n",
        "    sw = chi1/quad(self.chi_density, -np.inf, np.inf, args= (q))[0]\n",
        "    sb = q - sw * quad(self.qmap_density, -np.inf, np.inf, args= (q))[0]\n",
        "    return sw, sb\n",
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TXEAtO0oL4AT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Activation functions and their derivatives.\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1./(1 + np.exp(-x))\n",
        "\n",
        "def swish(x):\n",
        "  \"\"\"Swish function, normalized the derivative at the origin to be 1.\"\"\"\n",
        "  return 2 * x * sigmoid(x)\n",
        "\n",
        "def d_swish(x):\n",
        "  \"\"\"Derivative of swish.\"\"\"\n",
        "  return 2 * sigmoid(x) + 2 * swish(x) * (1 -  sigmoid(x))\n",
        "\n",
        "def d_tanh(x):\n",
        "  \"\"\"Derivative of tanh.\"\"\"\n",
        "  return 1./ np.cosh(x)**2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dq87r6zkKzFF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Phase diagram"
      ]
    },
    {
      "metadata": {
        "id": "7qANsoKoGFUF",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "background = [255.0/255.0, 229/255.0, 204/255.0]\n",
        "fontsize=16\n",
        "\n",
        "mf = MeanField(np.tanh, d_tanh)\n",
        "n = 50\n",
        "qrange=[0.05 * x for x in range(1, n+1)]\n",
        "\n",
        "sw = [mf.sw_sb(q, 1)[0] for q in qrange]\n",
        "sb = [mf.sw_sb(q, 1)[1] for q in qrange]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.plot(sw, sb)\n",
        "plt.xlim(0.5, 3)\n",
        "plt.ylim(0, 0.25)\n",
        "plt.title('Phase diagram', fontsize=fontsize)\n",
        "plt.xlabel('$\\sigma_\\omega^2$', fontsize=fontsize)\n",
        "plt.ylabel('$\\sigma_b^2$', fontsize=fontsize)\n",
        "\n",
        "plt.gca().set_axis_bgcolor(background)\n",
        "plt.gcf().set_size_inches(plt.gcf().get_size_inches()[1], plt.gcf().get_size_inches()[1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}